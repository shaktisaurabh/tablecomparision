{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2563ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first table sales_partner\n",
      "Enter the second table general_data\n",
      "This column KUNN2 of table1 and KUNNR of table2 matches can be used for joining\n",
      "The columns with object datatype are:\n",
      "['VKORG', 'VTWEG', 'PARVW', 'KUNN2', 'KUNNR', 'BU_GROUP', 'NAMORG1', 'NAMORG2', 'NAMORG3', 'NAME_FIRST', 'NAME_LAST', 'NAME_MIDDLE', 'SORTL', 'MCOD2', 'STREET', 'POST_CODE1', 'CITY1', 'COUNTRY', 'REGION', 'TIME_ZONE', 'POST_CODE2', 'LANGU_CORR', 'TELNR_LONG', 'FAXNR_LONG', 'SMTP_ADDR', 'SMTP_ADDR_2', 'SMTP_ADDR_3', 'column119', 'column120']\n",
      "\n",
      "\n",
      "The columns with int64 datatype are:\n",
      "['KTOKD', 'BU_ADEXT']\n",
      "\n",
      "\n",
      "The columns with float64 datatype are:\n",
      "['KUNNR', 'SPART']\n"
     ]
    }
   ],
   "source": [
    "tb1=input(\"Enter the first table \")\n",
    "tb2=input(\"Enter the second table \")\n",
    "\n",
    "df1=pd.read_sql(f'{tb1}',connn)\n",
    "df1.dropna(how='all',axis=1,inplace=True)#axis=1 removes columns that contains null values,how='all/any' specifies whether to remove\n",
    "#a column when all values are null or if any value is null\n",
    "df1\n",
    "df2=pd.read_sql(f'{tb2}',connn)#tb1 and tb2 are dynamically asked from user and then dynamically added to pd.read_sql using f_strings\n",
    "df2.dropna(how='all',axis=1,inplace=True)\n",
    "row1,colum1=df1.shape#this gives output in a way that the number of rows gets stored in row1 variable and number of columns get stored in column1 variable\n",
    "row2,column2=df2.shape\n",
    "lis=[]\n",
    "lis1=[]\n",
    "lis2=[]\n",
    "for colname in df1.columns:\n",
    "    \n",
    "    if(df1.dtypes[colname]=='object'):\n",
    "        lis.append(colname)\n",
    "    elif(df1.dtypes[colname]=='int64'):\n",
    "        lis1.append(colname)\n",
    "    elif(df1.dtypes[colname]=='float64'):\n",
    "        lis2.append(colname)\n",
    "for colname in df2.columns:\n",
    "    if(df2.dtypes[colname]=='object'):\n",
    "        lis.append(colname)\n",
    "    elif(df2.dtypes[colname]=='int64'):\n",
    "        lis1.append(colname)\n",
    "    elif(df2.dtypes[colname]=='float64'):\n",
    "        lis2.append(colname)\n",
    "    \n",
    "for i in range(colum1):\n",
    "    count=0\n",
    "    for j in range(column2):\n",
    "        if df1.dtypes[i]==df2.dtypes[j]:#here first datatypes are matched for each column of 2nd table w.r.t a given column of 1st table,and the respective columns of first table keeps on changing\n",
    "            \n",
    "            for z in range(row1):\n",
    "                if count>=int(row1/2) and count>=int(row2/2):#if the break statement over here followed by count and break statement below followed by count are not given then the print statement will keep getting executed again and again even after the count condition has been satisfied\n",
    "                    break\n",
    "                for k in range(row2):\n",
    "                    if df1.loc[z][i]==df2.loc[k][j]:\n",
    "                        count=count+1\n",
    "                        if count>=int(row1/2) and count>=int(row2/2):\n",
    "                            z=count\n",
    "                            print(f\"This column {df1.columns[i]} of table1 and {df2.columns[j]} of table2 matches can be used for joining\")\n",
    "                            break\n",
    "                                        \n",
    "            \n",
    "print(\"The columns with object datatype are:\")\n",
    "print(lis)\n",
    "print(\"\\n\")\n",
    "print(\"The columns with int64 datatype are:\")\n",
    "print(lis1)\n",
    "print(\"\\n\")\n",
    "print(\"The columns with float64 datatype are:\")\n",
    "print(lis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e38b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import pyodbc\n",
    "import sqlalchemy as sql\n",
    "import os\n",
    "import numpy as np\n",
    "connn=sql.create_engine('mssql+pyodbc://LAPTOP-B7AD4K08\\SQLEXPRESS/nothing?trusted_connection=yes&driver=ODBC Driver 17 for SQL Server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca825c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb1=input(\"Enter the first table \")\n",
    "tb2=input(\"Enter the second table \")\n",
    "\n",
    "df1=pd.read_sql(f'{tb1}',connn)\n",
    "df1.dropna(how='all',axis=1,inplace=True)\n",
    "df2=pd.read_sql(f'{tb2}',connn)\n",
    "df2.dropna(how='all',axis=1,inplace=True)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f7ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
